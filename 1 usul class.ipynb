{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6395f4-bc53-4467-86ee-4964ded78c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from collections import defaultdict\n",
    "from sklearn.covariance import MinCovDet\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8ca308-d5cc-4e53-ba56-7fcbcc67a513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('house_price_train.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd38b1e-9d44-40ce-b1f5-2af2870fdc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train = train_df.drop(columns=['SalePrice'])\n",
    "y_train = train_df['SalePrice']\n",
    "\n",
    "X_test = test_df.drop(columns=['SalePrice'])\n",
    "y_test = test_df['SalePrice']\n",
    "\n",
    "X_train.to_csv(\"X_train.csv\", index=False)\n",
    "y_train.to_csv(\"y_train.csv\", index=False)\n",
    "X_test.to_csv(\"X_test.csv\", index=False)\n",
    "y_test.to_csv(\"y_test.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b75ebd4-a0b0-48b2-98e4-887c9c08dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, X_train_path, y_train_path, X_test_path, y_test_path, run_default=False):\n",
    "        self.X_train = pd.read_csv(X_train_path)\n",
    "        self.y_train = pd.read_csv(y_train_path).squeeze()\n",
    "        self.X_test = pd.read_csv(X_test_path)\n",
    "        self.y_test = pd.read_csv(y_test_path).squeeze()\n",
    "\n",
    "        self.manual_ordinals = []\n",
    "        self.ordinal_mappings = {}\n",
    "\n",
    "        if run_default:\n",
    "            self.run_full_pipeline()\n",
    "\n",
    "    def run_full_pipeline(self,\n",
    "                          null_threshold=0.8,\n",
    "                          correlation_threshold=0.05,\n",
    "                          numeric_impute='median',\n",
    "                          categoric_impute='mode',\n",
    "                          outlier_method='zscore',\n",
    "                          scaler='standard',\n",
    "                          metrics=['r2', 'mae', 'mse']):\n",
    "        print(\"Running full preprocessing pipeline with default settings...\")\n",
    "\n",
    "        self.drop_uninformative_columns(null_threshold, correlation_threshold)\n",
    "        self.impute_missing_values(numeric_impute, categoric_impute)\n",
    "        self.detect_and_clean_outliers(outlier_method)\n",
    "        self.encode_features(self.manual_ordinals, self.ordinal_mappings)\n",
    "        self.scale_features(scaler)\n",
    "        self.train_and_evaluate_model(metrics)\n",
    "    def drop_uninformative_columns(self, null_threshold=0.8, correlation_threshold=0.05, multicol_thresh=0.75):\n",
    "        print(\"Dropping uninformative columns...\")\n",
    "\n",
    "        df = self.X_train.copy()\n",
    "        target = self.y_train.name if self.y_train.name else 'target'\n",
    "        df[target] = self.y_train\n",
    "\n",
    "        flagged = {}\n",
    "\n",
    "        # Null too high\n",
    "        null_ratios = df.isnull().mean()\n",
    "        flagged['TooManyNulls'] = null_ratios[null_ratios > null_threshold].index.tolist()\n",
    "\n",
    "        # Unique ID columns\n",
    "        flagged['UniqueIDColumns'] = [col for col in df.columns if df[col].nunique(dropna=False) == df.shape[0]]\n",
    "\n",
    "        # Low variance\n",
    "        flagged['LowVariance'] = [col for col in df.columns\n",
    "                                  if df[col].value_counts(normalize=True, dropna=False).values[0] > 0.99]\n",
    "\n",
    "        # Low correlation\n",
    "        numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "        if target in numeric_cols:\n",
    "            correlations = df[numeric_cols].corr()[target].abs()\n",
    "            low_corr_cols = correlations[correlations < correlation_threshold].index.tolist()\n",
    "            if target in low_corr_cols:\n",
    "                low_corr_cols.remove(target)\n",
    "            flagged['LowCorrelation'] = low_corr_cols\n",
    "        else:\n",
    "            flagged['LowCorrelation'] = []\n",
    "\n",
    "        # Redundant combinations: A + B = C\n",
    "        redundant_combos = []\n",
    "        to_drop_redundant = []\n",
    "        valid_cols = [col for col in numeric_cols if df[col].nunique() >= 5 and (df[col] == 0).mean() <= 0.8]\n",
    "\n",
    "        for col1, col2 in combinations(valid_cols, 2):\n",
    "            summed = df[col1].fillna(0) + df[col2].fillna(0)\n",
    "            for col3 in valid_cols:\n",
    "                if col3 in (col1, col2): continue\n",
    "                match_ratio = np.isclose(summed, df[col3].fillna(0), rtol=0.01, atol=1).mean()\n",
    "                if match_ratio > 0.98:\n",
    "                    redundant_combos.append((col1, col2, col3))\n",
    "                    to_drop_redundant.append(col3)\n",
    "\n",
    "        flagged['RedundantToDrop'] = list(set(to_drop_redundant))\n",
    "\n",
    "        # Multicollinearity\n",
    "        corr_matrix = df[numeric_cols].corr().abs()\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop_multi = []\n",
    "\n",
    "        for col in upper_tri.columns:\n",
    "            for row in upper_tri.index:\n",
    "                if col == target or row == target:\n",
    "                    continue\n",
    "                corr_val = upper_tri.loc[row, col]\n",
    "                if pd.notnull(corr_val) and corr_val > multicol_thresh:\n",
    "                    to_drop_multi.append(row if corr_matrix[target][row] < corr_matrix[target][col] else col)\n",
    "\n",
    "        flagged['MulticollinearityToDrop'] = list(set(to_drop_multi))\n",
    "\n",
    "        cols_to_drop = set(sum(flagged.values(), []))\n",
    "        self.X_train = self.X_train.drop(columns=cols_to_drop, errors='ignore')\n",
    "        self.X_test = self.X_test.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "        self.flags = flagged\n",
    "        print(f\"Dropped {len(cols_to_drop)} columns.\")\n",
    "\n",
    "        return flagged\n",
    "    def impute_missing_values(self, strategy_numeric='median', strategy_categoric='mode'):\n",
    "        print(\"The imputation process has begun...\")\n",
    "\n",
    "        X_train = self.X_train.copy()\n",
    "        X_test = self.X_test.copy()\n",
    "\n",
    "        numeric_cols = X_train.select_dtypes(include=np.number).columns\n",
    "        categoric_cols = X_train.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            missing_ratio = X_train[col].isnull().mean()\n",
    "\n",
    "            if strategy_numeric == 'median':\n",
    "                fill_value = X_train[col].median()\n",
    "            elif strategy_numeric == 'mean':\n",
    "                fill_value = X_train[col].mean()\n",
    "            elif strategy_numeric == 'constant':\n",
    "                fill_value = 0\n",
    "            elif strategy_numeric == 'missing_flag':\n",
    "                X_train[col + '_missing_flag'] = X_train[col].isnull().astype(int)\n",
    "                X_test[col + '_missing_flag'] = X_test[col].isnull().astype(int)\n",
    "                fill_value = X_train[col].median()\n",
    "            else:\n",
    "                raise ValueError(f\"Naməlum numeric imputation strategiyası: {strategy_numeric}\")\n",
    "\n",
    "            X_train[col] = X_train[col].fillna(fill_value)\n",
    "            X_test[col] = X_test[col].fillna(fill_value)\n",
    "\n",
    "        for col in categoric_cols:\n",
    "            if strategy_categoric == 'mode':\n",
    "                fill_value = X_train[col].mode(dropna=True)[0] if not X_train[col].mode(dropna=True).empty else 'Missing'\n",
    "            elif strategy_categoric == 'missing':\n",
    "                fill_value = 'Missing'\n",
    "            elif strategy_categoric == 'constant':\n",
    "                fill_value = 'Unknown'\n",
    "            else:\n",
    "                raise ValueError(f\"Naməlum categoric imputation strategiyası: {strategy_categoric}\")\n",
    "\n",
    "            X_train[col] = X_train[col].fillna(fill_value)\n",
    "            X_test[col] = X_test[col].fillna(fill_value)\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "\n",
    "        print(\"Imputation completed.\")\n",
    "    def detect_and_clean_outliers(self, method='zscore'):\n",
    "        print(\"Outlier removal started...\")\n",
    "\n",
    "        X = self.X_train.copy()\n",
    "        y = self.y_train.copy()\n",
    "        numeric_cols = X.select_dtypes(include='number').columns\n",
    "\n",
    "        valid_cols = [col for col in numeric_cols if X[col].nunique() >= 10]\n",
    "\n",
    "        if method == 'zscore':\n",
    "            mask = np.ones(len(X), dtype=bool)\n",
    "            for col in valid_cols:\n",
    "                z_scores = np.abs(stats.zscore(X[col].fillna(X[col].median())))\n",
    "                mask &= (z_scores < 3)\n",
    "            self.X_train = X[mask].reset_index(drop=True)\n",
    "            self.y_train = y[mask].reset_index(drop=True)\n",
    "\n",
    "        elif method == 'iqr':\n",
    "            mask = np.ones(len(X), dtype=bool)\n",
    "            for col in valid_cols:\n",
    "                Q1 = X[col].quantile(0.25)\n",
    "                Q3 = X[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower = Q1 - 1.5 * IQR\n",
    "                upper = Q3 + 1.5 * IQR\n",
    "                mask &= X[col].between(lower, upper)\n",
    "            self.X_train = X[mask].reset_index(drop=True)\n",
    "            self.y_train = y[mask].reset_index(drop=True)\n",
    "\n",
    "        elif method == 'modified_z':\n",
    "            mask = np.ones(len(X), dtype=bool)\n",
    "            for col in valid_cols:\n",
    "                median = X[col].median()\n",
    "                mad = np.median(np.abs(X[col] - median))\n",
    "                if mad == 0:\n",
    "                    continue\n",
    "                mod_z = 0.6745 * (X[col] - median) / mad\n",
    "                mask &= (np.abs(mod_z) < 3.5)\n",
    "            self.X_train = X[mask].reset_index(drop=True)\n",
    "            self.y_train = y[mask].reset_index(drop=True)\n",
    "\n",
    "        elif method == 'isolation_forest':\n",
    "            from sklearn.ensemble import IsolationForest\n",
    "            model = IsolationForest(contamination=0.01, random_state=42)\n",
    "            preds = model.fit_predict(X[valid_cols].fillna(0))\n",
    "            mask = preds != -1\n",
    "            self.X_train = X[mask].reset_index(drop=True)\n",
    "            self.y_train = y[mask].reset_index(drop=True)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unspecified outlier method: {method}\")\n",
    "\n",
    "        print(f\"Outlier removal completed. Remaining number of observations: {len(self.X_train)}\")\n",
    "    def encode_features(self, manual_ordinals=None, ordinal_mappings=None):\n",
    "        print(\"Encoding stage started...\")\n",
    "\n",
    "        if manual_ordinals is not None:\n",
    "            self.manual_ordinals = manual_ordinals\n",
    "        if ordinal_mappings is not None:\n",
    "            self.ordinal_mappings = ordinal_mappings\n",
    "\n",
    "        X_train = self.X_train.copy()\n",
    "        X_test = self.X_test.copy()\n",
    "\n",
    "        manual_ordinals = self.manual_ordinals\n",
    "        ordinal_mappings = self.ordinal_mappings\n",
    "\n",
    "        categorical_cols = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "        label_encoding_cols = []\n",
    "        one_hot_encoding_cols = []\n",
    "        ordinal_encoding_cols = [col for col in manual_ordinals if col in categorical_cols]\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            if col in ordinal_encoding_cols:\n",
    "                continue\n",
    "            nunique = X_train[col].nunique()\n",
    "            if nunique <= 2:\n",
    "                label_encoding_cols.append(col)\n",
    "            else:\n",
    "                one_hot_encoding_cols.append(col)\n",
    "\n",
    "        from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "        # Label encoding\n",
    "        for col in label_encoding_cols:\n",
    "            le = LabelEncoder()\n",
    "            X_train[col] = le.fit_transform(X_train[col])\n",
    "            X_test[col] = X_test[col].map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
    "\n",
    "        # Ordinal encoding\n",
    "        for col in ordinal_encoding_cols:\n",
    "            if col not in ordinal_mappings:\n",
    "                continue\n",
    "            mapping = ordinal_mappings[col]\n",
    "            enc = OrdinalEncoder(categories=[mapping], handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "            X_train[col] = enc.fit_transform(X_train[[col]])\n",
    "            X_test[col] = enc.transform(X_test[[col]])\n",
    "\n",
    "        # One-hot encoding\n",
    "        if one_hot_encoding_cols:\n",
    "            ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "            ohe.fit(X_train[one_hot_encoding_cols])\n",
    "\n",
    "            train_encoded = pd.DataFrame(\n",
    "                ohe.transform(X_train[one_hot_encoding_cols]),\n",
    "                columns=ohe.get_feature_names_out(one_hot_encoding_cols),\n",
    "                index=X_train.index\n",
    "            )\n",
    "\n",
    "            test_encoded = pd.DataFrame(\n",
    "                ohe.transform(X_test[one_hot_encoding_cols]),\n",
    "                columns=ohe.get_feature_names_out(one_hot_encoding_cols),\n",
    "                index=X_test.index\n",
    "            )\n",
    "\n",
    "            X_train = X_train.drop(columns=one_hot_encoding_cols).join(train_encoded)\n",
    "            X_test = X_test.drop(columns=one_hot_encoding_cols).join(test_encoded)\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "\n",
    "        print(\"Encoding completed.\")\n",
    "    def scale_features(self, scaler='standard'):\n",
    "        print(\"Scaling started...\")\n",
    "\n",
    "        from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "        X_train = self.X_train.copy()\n",
    "        X_test = self.X_test.copy()\n",
    "\n",
    "        numeric_cols = X_train.select_dtypes(include=np.number).columns\n",
    "\n",
    "        if scaler == 'standard':\n",
    "            scaler_obj = StandardScaler()\n",
    "        elif scaler == 'minmax':\n",
    "            scaler_obj = MinMaxScaler()\n",
    "        elif scaler == 'robust':\n",
    "            scaler_obj = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Tanınmayan scaler növü: {scaler}\")\n",
    "\n",
    "        scaler_obj.fit(X_train[numeric_cols])\n",
    "\n",
    "        X_train[numeric_cols] = scaler_obj.transform(X_train[numeric_cols])\n",
    "        X_test[numeric_cols] = scaler_obj.transform(X_test[numeric_cols])\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        \n",
    "        print(\"Scaling completed.\")\n",
    "    def train_and_evaluate_model(self, metrics=None):\n",
    "        print(\"Model training and evaluation started...\")\n",
    "\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "        X_train = self.X_train\n",
    "        X_test = self.X_test\n",
    "        y_train = self.y_train\n",
    "        y_test = self.y_test\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "\n",
    "        available_metrics = {\n",
    "            \"r2\": r2_score,\n",
    "            \"mae\": mean_absolute_error,\n",
    "            \"mse\": mean_squared_error\n",
    "        }\n",
    "\n",
    "        if metrics is None:\n",
    "            metrics = [\"r2\", \"mae\", \"mse\"]\n",
    "\n",
    "        print(\"Train results:\")\n",
    "        for name in metrics:\n",
    "            if name in available_metrics:\n",
    "                value = available_metrics[name](y_train, y_pred_train)\n",
    "                print(f\"  {name.upper()}: {value:.4f}\")\n",
    "\n",
    "        print(\"\\nTest results:\")\n",
    "        for name in metrics:\n",
    "            if name in available_metrics:\n",
    "                value = available_metrics[name](y_test, y_pred_test)\n",
    "                print(f\"  {name.upper()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f96f7dd-7021-47a0-af4e-cf567d5815f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running full preprocessing pipeline with default settings...\n",
      "Dropping uninformative columns...\n",
      "Dropped 23 columns.\n",
      "The imputation process has begun...\n",
      "Imputation completed.\n",
      "Outlier removal started...\n",
      "Outlier removal completed. Remaining number of observations: 998\n",
      "Encoding stage started...\n",
      "Encoding completed.\n",
      "Scaling started...\n",
      "Scaling completed.\n",
      "Model training and evaluation started...\n",
      "Train results:\n",
      "  R2: 0.9341\n",
      "  MAE: 11846.7816\n",
      "  MSE: 278881837.5721\n",
      "\n",
      "Test results:\n",
      "  R2: 0.8627\n",
      "  MAE: 19066.3597\n",
      "  MSE: 1053037426.8743\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dp = DataPreprocessor(\n",
    "    X_train_path=\"X_train.csv\",\n",
    "    y_train_path=\"y_train.csv\",\n",
    "    X_test_path=\"X_test.csv\",\n",
    "    y_test_path=\"y_test.csv\"\n",
    ")\n",
    "\n",
    "dp.run_full_pipeline(\n",
    "    null_threshold=0.7,\n",
    "    correlation_threshold=0.1,\n",
    "    numeric_impute='median',\n",
    "    categoric_impute='mode',\n",
    "    outlier_method='zscore',\n",
    "    scaler='standard',\n",
    "    metrics=['r2', 'mae', 'mse']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f2213-ebc4-4737-8c0e-6ab90a6f3eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41798e32-75a3-4358-9180-e37b3e33f928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
